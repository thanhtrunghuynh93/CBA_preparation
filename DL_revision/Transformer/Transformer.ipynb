{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585e1b9e-829d-464b-9eb3-ae3ab298ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.datasets import IMDB\n",
    "from collections import Counter\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3d1d16-586b-4610-b996-a9d50be84148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def build_vocab(data, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for label, line in data:\n",
    "        counter.update(tokenize(line))\n",
    "    most_common = counter.most_common(max_size - 2)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    vocab.update({word: i+2 for i, (word, _) in enumerate(most_common)})\n",
    "    return vocab\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=200):\n",
    "        self.data = []\n",
    "        for label, text in data:\n",
    "            tokens = tokenize(text)\n",
    "            ids = [vocab.get(token, 1) for token in tokens[:max_len]]\n",
    "            ids += [0] * (max_len - len(ids))\n",
    "            label = 1 if label == 'pos' else 0\n",
    "            self.data.append((torch.tensor(ids), torch.tensor(label)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c4741c-f673-4aea-9a53-5c775e6cd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Positional Encoding (reuse)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Multi-head Attention (self and cross)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B, T_q, D = query.shape\n",
    "        T_k = key.size(1)\n",
    "\n",
    "        Q = self.q_proj(query).reshape(B, T_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).reshape(B, T_k, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).reshape(B, T_k, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim ** 0.5  # (B, heads, T_q, T_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)  # (B, heads, T_q, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().reshape(B, T_q, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x, x, x, src_mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "# Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_out, enc_out, memory_mask)))\n",
    "        x = self.norm3(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "# Full Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, max_len=512):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(embed_dim, num_heads, ff_hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, num_heads, ff_hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        T = tgt.size(1)\n",
    "        mask = torch.tril(torch.ones(T, T, device=tgt.device)).unsqueeze(0).unsqueeze(1)\n",
    "        return mask  # (1, 1, T, T)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None):\n",
    "        src = self.pos_enc(self.src_embed(src))\n",
    "        tgt = self.pos_enc(self.tgt_embed(tgt))\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, src, tgt_mask, src_mask)\n",
    "\n",
    "        return self.output_proj(tgt)  # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a10798-e3b3-4fc1-b054-81c7fed143b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
